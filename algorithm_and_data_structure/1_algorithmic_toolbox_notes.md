Courseraの以下のコースを受講した際のノートです
Algorithmic Toolbox
Data Structures and Algorithms
by University of California, San Diego & Higher School of Economics

#Algorithmic Toolbox

[TOC]


##week1
イントロなので省略します。

##week2
###アルゴリズムの指標
week2はアルゴリズムの基礎である、実行時間の理論的定義を学びます。
アルゴリズムの性能を図る指標としては

- 時間計算量（Time Complexity, 領域計算量とも呼ばれます）
- 空間計算量（Space Complexity）

の２つがあり、このコースでは主に時間計算量、言い換えるとアルゴリズムの実行時間（Running Time）を求め、アルゴリズムの性能を測定する根拠とします。

空間計算量 (Space Complexity)の定義
> 空間計算量 【 space complexity 】 領域計算量
空間計算量とは、コンピュータが特定の手順に従って与えられた問題を解く際に必要とする記憶領域の容量。これが少ないほど、より少ないメモリ容量で問題を解くことができる。
(http://e-words.jp/w/%E7%A9%BA%E9%96%93%E8%A8%88%E7%AE%97%E9%87%8F.html)
コンピュータにおける実行時間の計測はさまざまな要因が入り組み困難を極めます。例えば、

- CPUのベンチマーク
- システムアーキテクチャ
- コンパイラ／インタープリタの処理速度
- メモリー階層（複数のキャッシュやスワップなど）
- コンピュータを使用しているユーザやプロセスの数

などなど数え切れない要因があり、性格な実行時間を計測するのは難しい。
しかし、なんとかして”わかりやすく”、”根拠のある”方法でアルゴリズムの実力を比べることはできないか。
そんなときに必要になるのが、漸次的記法です。

###漸次的記法（Asymptotic Notation）

前提
・細かいことは気にせず実行時間を数量化
・大きいインプットの場合で近似する

基本的な考え方
インプットの大きさに"応じて"実行時間はどのように変化するか
>How does runtime "scale" with input size



(week1のスライドから引用）

例えば、

|input size|n|nlogn|n^2|2^n|
|---|---|---|---|---|
|n = 20|1 sec|1 sec|1 sec|1 sec|
|n = 50|1 sec|1 sec|1 sec|13 days|
|n = 10^2|1 sec|1 sec|1 sec|4*10^13 year|
|n = 10^6|1 sec|1 sec|17 mins||
|n = 10^9|1 sec|30 sec|10 years||

のようにインプットnの大きさに応じて、実行時間が変化するような記法を定義します。

なお、漸次的記法における実行時間の計算方法は以下に分類されるそうです。

1.置き換え法(Substitution Method)
1.再帰木法(Recursion Tree Method)
1.分類法(Master Theorem Method)

（http://hrk623.hatenablog.com/entry/20110720）

###O記法 (Bit-O Notation)
漸次的記法のひとつであり、おそらく最もよく使われているものです。

定義
$$$f(n) = O(g(n))$$$
($$$f$$$ is Big-O of $$$g$$$) or $$$f \preceq g$$$ if there exist constants $$$N$$$ and $$$c$$$ so that for all $$$n \geq N$$$, $$$f(n) \leq cg(n)$$$

日本語だと以下のような定義になります。
>nが整数であるとします。
f(n)=O(g(n)) であるとは、整数 m と正の定数 c が存在して、n≧m である任意の n について f(n)≦cg(n) であることです。いいかえると、ｎが小さい有限個の例外と、nに依存しない定数の影響を除いて、 g(n) は常に f(n) より大きい、といいたいわけです。このとき、f(n) は オーダー g(n) であるといいます。

(http://www.triplefalcon.com/Lexicon/Notation-BigO-1.htm)

wiki pediaによるとこれはランダウの記号とも呼ばれているらしく、以下のことが書いてありました。
>変数 x を極限に飛ばした時の関数 f の振る舞い（漸近的挙動）を、別の関数 g を目安にして記述する目的で用いられる。

(https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%B3%E3%83%80%E3%82%A6%E3%81%AE%E8%A8%98%E5%8F%B7)


簡単に言ってしまえば、f(n) の成長はg(n)と同じかそれより遅い。ということです。
(nを極限に飛ばしたときの大小関係と同じです。)

####ルール
1. *定数による乗算は無視する*

	$$
	7n^3 = O(n^3)\\
    \frac{n^3}{3} = O(n^2)
	$$

1. $$$n^a \preceq n^b$$$ for $$$0 < a < b$$$

	$$
	n = O(n^2)\\
    \sqrt{2} = O(n)
	$$

1. $$$n^a \preceq n^b (a > 0, b > 1)$$$

	$$
	n^5 = O(\sqrt{n^2})\\
    n^{100} = O(1.1^n)
	$$

1. $$$(\log{}n)^a \preceq n^b (a, b > 0)$$$

	$$
	(\log{}n)^3 = O(\sqrt{n})\\
    n\log{}n = O(n^2)
	$$

1. *より小さな項は無視する*

	$$
	n^2 + n = O(n^2)\\
    2^n + n^9 = O(2^n)
	$$


それではBig-Oを使用して以下のルーティーンの実行速度を測定してみます。

```
create an array F[0,...,n] // O(n)
F[0] = 0 // O(1)
F[1] = 1 // O(1)
for i from 2 to n
    F[i] = F[i-1] + F[i-2] // O(n) (※1)
return F[n] // O(1)
```
(※1)ここで`F[i] = F[i-1] + F[i-2]`という処理の実行時間は一見$$$O(1)$$$に感じますが、加算する数値が十分に大きい場合、１つの演算では収まらず、繰り上げ（キャリー）が発生するため、$$$O(n)$$$になります。（おそらく、講義ではそのようなことを言っていました）

上記のルーティーンの実行速度は

$$
O(n) + O(1) + O(1) + O(n)*O(n) + O(1) = O(n^2)
$$

となります。

ちなみに、よくでてくる実行時間の比較はこんな感じです。
$$
\log{}n \preceq \sqrt{n} \preceq n \preceq n\log{}n \preceq n^2 \preceq 2^n
$$

###その他の漸次的記法
O記法の他にもまあまあの頻度で使用される記法には以下のものがあります。

####Ω記法 (Omega Notation)
f(n) の発散のスピードは遅くても g(n) と同じくらい

For functions $$$f , g : N → R^+ $$$
we say that:
f (n) = Ω(g (n)) or f ⪰ g if for some c,
f (n) ≥ c · g (n) (f grows no slower than g ).

####Θ記法 (Theta Notation)
f(n) はだいたい g(n) と同じスピードで発散する

For functions $$$f , g : N → R^+ $$$
we say that:
f (n) = Θ(g (n)) or f ≍ g if f = O(g )
and f = Ω(g ) (f grows at the same rate as g ).

####o記法 (Little-o Notation)
f(n) の発散のスピードは g(n) より遅い

For functions $$$f , g : N → R^+ $$$
we say that:

f (n) = o(g (n)) or f ≺ g if
f (n)/g (n) → 0 as n → ∞ (f grows slower than g )

(参考：http://mathtrain.jp/landausymbol)

##week3
###Greedy Algorithm
その名の通り、「貪欲なアルゴリズム」です。
簡単なフローは以下の通りです。

1. 貪欲な処理をする（Make a greedy choice）
1. その処理が正しいか確かめる（Prove that it is a safe move）
1. より小さな問題に変更する（Reduce to a subproblem）
1. 問題を解決する（Solve the subproblem）

上記の４つを繰り返し行うことにより問題を解いていきます。
ここでは例として[5, 8, 9, 3, 2]という配列を降順にソートしていくアルゴリズムを通してGreedy Algorithmがどのようなものか見ていきます。

1,2.まず、もっとも大きい数を取り出して別の配列の一番最初に格納します。
```
[5, 8, 9, 3, 2], [9,,,,]
```

3.次に与えられた配列から選んだ番号を削除します。
```
[5, 8,, 3, 2], [9,,,,]
```

4.この段階で与えられた配列は最初の配列のサブセットとなっており、あとは1〜3を繰り返すことで以下のように変数が変化します。
```
[5,,, 3, 2], [9, 8,,,]
[,,, 3, 2], [9, 8, 5,,]
[,,,, 2], [9, 8, 5, 3,]
[,,,,], [9, 8, 5, 3, 2]
```


##week4
###Divide and Conquer
week4はバイナリサーチなどに代表される「分割統治法」です。

###Polynomial Multiplication

###マスター定理（Master Theorem）
レクチャー以外に参考にしたもの
（http://hrk623.hatenablog.com/entry/20110720）

再帰的呼び出しを伴うアルゴリズムの漸次的記法を計算するための方法です。

ここで、$$$T(n)$$$は計算量とします。

$$
if \ T(n) = aT(\frac{n}{b}) \ + \ O(n^{d}) \\
(for \ constants \ a > 0,\ b > 1, d \geq 0),\ then \\
\begin{eqnarray}
T(n) = \left\{ \begin{array}{ll}
O(n^{d}), \hspace{3em}  if \ d > \log_{b} a \ \ ... (1)\\
O(n^{d}\log{} n), \hspace{1em} if \ d = \log_{b} a \ \ ... (2)\\
O(n^{\log{b} a}), \hspace{2em} if \ d < \log_{b} a \ \ ... (3)\\
\end{array} \right.
\end{eqnarray}
$$

####証明

再帰的な呼び出しが何回あるかに寄って計算が変わっていきます。
回数を$$$n$$$とすると

n = 0の場合
$$
O(n^{d})
$$

n = 1の場合
$$
aO(\frac{n}{b})^{d} = O(n^{d})\frac{a}{b^d}
$$

n = iの場合
$$
a^{i}O(\frac{n}{b^i})^{d} \ = O(n^d)(\frac{a}{b^d})^i
$$


###選択ソート（Selection Sort）
まず始めに結論から書くと、最適化されたソートアルゴリズムの実行時間は$$$O(n\log{n})$$$です。それを踏まえてこれから出てくるいくつかのアルゴリズムを見ていきます。

ソートのアルゴリズムのなかでも特に単純なのがこの選択ソートです。
考え方は単純で、ソートする配列の要素を順にチェックしていき、すべての中で最も小さい値の要素を一番左の要素を交換します。
よってｎ個の要素を持つ配列をソートする実行時間は
$$
n + (n-1) + ... + 1
$$
となり、等差級数の公式
$$
1 + 2 + ... + n = \frac{n(n+1)}{2}
$$
から$$$O(\frac{n(n+1)}{2})$$$となりそうですが、O記法の決まりによって定数は省略され、
$$
O(n^2)
$$
となります。
選択ソートの他にも実行時間が2乗になるアルゴリズムはバブルソートや挿入ソートなどがあります。

###マージソート（Merge Sort）
マージソートはソート対象のオブジェクトについての情報がなにもない場合のソートアルゴリズムとして、最適である$$$O(n\log{}n)$$$の実行時間で処理を完了することができるアルゴリズムです。その方法は要素を２要素になるまで２分割していき、１〜２要素まで分割したら比較・入れ替えを行い、また分割しながら比較していきます。

マージソートの疑似コードは以下のようになります。

```
MergeSort(A[1 . . . n]) {
    if n = 1:
        return A
    m ← ⌊n/2⌋
    B ← MergeSort(A[1 . . . m])			//T(n)
    C ← MergeSort(A[m + 1 . . . n])		//T(n)
    A ′ ← Merge(B, C )					//O(n)
    return A ′
}
```
```
Merge(B[1 . . . p], C [1 . . . q]) {
    D ← empty array of size p + q
    while B and C are both non-empty:
        b ← the first element of B
        c ← the first element of C
        if b ≤ c:
            move b from B to the end of D
        else:
            move c from C to the end of D
    move the rest of B and C to the end of D
    return D
}
```

Merge(B, C)の実行時間は$$$O(n)$$$なので、MergeSort(A)の実行時間は$$$T(n)\geq2T(n/2) + O(n)$$$となり、マスター定理の(2)より、
$$
O(n\log{}n)
$$
となります。


###クイックソート（Quick Sort）

```
/*
* クイックソート
* @param A	配列
* @param l	ソートの開始要素
* @param r	ソートの終了要素
*/
QuickSort(A, l, r )
    if l ≥ r :
        return
    m ← Partition(A, l, r )
    {A[m] is in the final position}
    QuickSort(A, l, m − 1)
    QuickSort(A, m + 1, r )
```

```
Partition(A, l, r )
    x ← A[l]					//pivot
    j ← l
    for i from l + 1 to r :
        if A[i] ≤ x:
            j ← j + 1
            swap A[j] and A[i]	//{A[l + 1 . . . j] ≤ x < A[j + 1 . . . i]
    swap A[l] and A[j]
    return j
```
####ピボットの重要性
ここで、あるピボットによって、アンバランスに分割された例を見てみます。

1. 
$$
T (n) = n + T (n − 1)\\
T (n) = n+(n−1)+(n−2)+· · · = Θ(n^2)
$$

1. 
$$
T (n) = n + T (n − 5) + T (4)\\
T (n) ≥ n+(n−5)+(n−10)+· · · = Θ(n^2)
$$

1.はかなりアンバランスで要素が１つの配列とそれ以外の配列で分割されます。この場合、計算時間は$$$Θ(n^2)$$$となります。
2.は1.よりは改善されましたが、計算時間は同じく$$$Θ(n^2)$$$となります。

上の２つの例とは違い、与えられた配列を任意のiで等分できたときを考えてみます。
このとき、計算時間は
$$
T (n) = 2T (n/2) + n\\
T (n) = Θ(n\log{}n)
$$

また、
$$
T (n) = T (n/10) + T (9n/10) + n\\
T (n) = Θ(n\log{}n)
$$
になります。

ピボットは中央値を取るのがベスト

####Random Pivot
lからrまでの要素のなかでランダムに１つ選択し、ピボットにする。
ランダムに選んでも、中心に近い値を選ぶ確率は半分であり、なかなか良い。

####証明
#####比較
- 実行時間は値を比較する数に比例する
- 均等な分割は値の比較を最小限に抑えることができる

#####任意の２つの要素が比較される確率
任意の２つ要素のインデックスを$$$i$$$, $$$j$$$とすると、対応する要素$$$A'[i]$$$, $$$A'[j]$$$が比較される確率は以下のようになる

- let, for $$$i < j,$$$
    $$
    \begin{eqnarray}
    X_{ij} = \left\{ \begin{array}{ll}
    1 \ A ′ [i] \ and  \ A ′ [j] \ are \ compared \ A \\
    0 \ otherwise \\
    \end{array} \right.
    \end{eqnarray}
    $$

- すべての $$$i < j$$$, $$$A ′ [i]$$$ と $$$A ′ [j]$$$ は１回だけ比較されるか比較されない（ピボットと比較される）かのどちらか。
- これは最悪のケースでは実行時間が $$$O(n^2)$$$ となることを示す


よって、比較される確率$$$Prob(X_{ij})$$$ は
$$
Prob(X_{ij}) = \frac{2}{j-i+1}
$$
となり、任意の２つ要素を比較する実行時間の期待値 $$$E(X_{ij})$$$ は
$$
E(X_{ij}) = \frac{2}{j-i+1}
$$
となる。

これらからアルゴリズム全体の実行時間の期待値を算出すると
$$
E\sum_{i=1}^{n}\sum_{j=i+1}^{n}X_{ij} \\
= \sum_{i=1}^{n}\sum_{j=i+1}^{n}E(X_{ij}) \\
= \sum_{i < j}{}\frac{2}{j-i+1} \\
\leq 2n \cdot (\frac{1}{2} + \frac{1}{3} + \cdot \cdot \cdot + \frac{1}{n}) \\
= \Theta(n\log{}n)
$$
となる。

####Equal Elements

####末尾呼び出しの除去 (Tail Call Elimination)

関数の内部で自身を呼び出すと関数のデータがスタックフレームとしてメモリに蓄積されていきます。

コールスタック(Call Stack)

> 通常のプログラムの実行モデルでは、関数を呼び出すと、関数自身のローカル変数（引数や局所変数）や戻り先情報を保持するスタックフレーム（アクティベーションレコード）が生成されてコールスタックと呼ばれるスタックにpushされます。関数はスタックフレームに計算途中の値を保持しつつ処理を続けます。関数の実行が終わると対応するスタックフレームがpopされて呼び出し元のスタックフレームに復帰し、呼び出し元関数の実行が再開されます。
(http://qiita.com/pebblip/items/cf8d3230969b2f6b3132#%E6%9C%AB%E5%B0%BE%E5%91%BC%E3%81%B3%E5%87%BA%E3%81%97%E3%81%A8%E6%9C%AB%E5%B0%BE%E5%91%BC%E3%81%B3%E5%87%BA%E3%81%97%E6%9C%80%E9%81%A9%E5%8C%96)

このメモリ消費量は空間計算量を大きくし、ある一定数を超えるとスタックオーバーフローにもなりかねません。
そのため、プログラム最適化の１つの方法である「末尾呼び出しの除去」を使用し、スタックに蓄積されるデータ量を減らします。

末尾呼び出しの除去 (Tail Call Elimination)

> 上記のsquareSum関数は、スタックフレームに計算途中の値（xsやresult）を保持して実行を続けます。このように、スタックフレームは関数の実行途中の値を保持する一時領域としての役割を持ちますが、再帰関数の例で既にみたように、関数呼び出し階層があまりにも深くなるとスタックフレーム数が大きくなり、メモリ領域を食いつぶしてしまうことがあります。不要なスタックフレームの生成はできれば避けたいところです。
> スタックフレームは関数の計算途中の値を保持するのに必要ですが、言い換えると、計算途中の値がそれ以上使用されないのであれば、スタックフレームも不要ということになります。ここで、関数の末尾呼び出しに着目してみましょう。ある関数fが別の関数gを末尾呼び出ししているとき、fの結果（リターン値）はgの結果そのものです。つまり、gの呼び出し後にfのスタックフレームは使用されません。したがって、gの呼び出し時にスタックフレームを新たに生成するのではなく、fのスタックフレームをgのスタックフレームとして再利用することが可能です。これを 末尾呼び出しの除去（tail call elimination） と呼びます。コンパイラや実行環境が行う末尾呼び出しの除去を 末尾呼び出し最適化（tail call optimization） といいます。
(http://qiita.com/pebblip/items/cf8d3230969b2f6b3132#%E6%9C%AB%E5%B0%BE%E5%91%BC%E3%81%B3%E5%87%BA%E3%81%97%E3%81%A8%E6%9C%AB%E5%B0%BE%E5%91%BC%E3%81%B3%E5%87%BA%E3%81%97%E6%9C%80%E9%81%A9%E5%8C%96)
```

クイックソートにおいて、この末尾呼び出しの除去を使用する際に気をつけておくべき点は分割した２つの配列の内、長さが大きい方のスタックを除去することです。末尾呼び出しにできる関数は２つの再帰関数の内どちらかなので、スタックを多く使用する方を除去することはより小さな空間計算量を実現することになります。
以下が末尾呼び出し除去を取り入れた擬似コードになります。

```
QuickSort(A, l, r )
    while l < r :
        m ← Partition(A, l, r )
        if (m − l) < (r − m):			//短い方
            QuickSort(A, l, m − 1)
            l ← m + 1
        else:							//長い方
            QuickSort(A, m + 1, r )
            r ← m − 1
```

##week5

###動的計画法 (Dynamic Programming)
動的計画法は簡潔にいうと以下の2つの要素を持つアルゴリズムです。
>分割統治法：部分問題を解き、その結果を利用して、問題全体を解く
メモ化：部分問題の計算結果を再利用する


https://ja.wikipedia.org/wiki/%E5%8B%95%E7%9A%84%E8%A8%88%E7%94%BB%E6%B3%95

####ナップザック問題
動的計画法を使用する有名な問題に「ナップザック問題」というものがあります。
重さ$$$W$$$まで入れることができるナップザックに$$$n$$$個の品物（重さ$$$w_i$$$、価値$$$v_i$$$）を詰めていき、最終的なナップザックに入っている品物の合計価値$$$V$$$を最大にするにはどの品物をどのくらいずつナップザックに詰めればよいかという問題です。

ナップザック問題にはいくつかのバリエーションがあります。
- 分割可能（fractional knapsack: 与えられてた品物は適当な重さに分割可能）
- 分割不可能（discrete knapsack: 与えられた品物は分割不可能であり、そのままの重さでナップザックに詰める必要がある）
  - 品物が無限（knapsack with repetitions: 与えられた品物は何個でも使用可能）
  - 品物が有限（knapsack without repetitions: 与えられた品物は1つまで使用可能）

ここでは分割不可能な問題について考えていきます。

#####品物が無限の場合 (Knapzack with repetition)
この問題を特には
- すべての可能性を検証する（全探索）
- 単位重量あたりの価値の高い品物から順に詰めていく（貪欲法）

といった方法が考えられますが、全探索では実行時間が$$$2^n$$$という膨大な時間が掛かってしまいます。
それでは貪欲法ではどうかというと、この方法では実は最適な解を見つけることができない場合があります。
例えば、品物$$$g_i$$$（重さ$$$w_i$$$、価値$$$v_i$$$）とすると
$$$g_1(6, 30), \ g_2(3, 14), \ g_3(4, 16), \ g_4(2, 9)$$$
の4つの品物と重さ10まで入れることができるナップザックがあるとします。
この場合、最適解$$$V$$$は貪欲法では答えが価値44となりますが、実際は48という最適解が存在し、貪欲法だけでは最適解を求めることができません。（この問題はNP困難と呼ばれているそうです。）

この2つの問題を解決するために、動的計画法を用いて以下のようなアルゴリズムを考えます。
まず、基本は全探索っぽく説いていきますが、途中の値を保存し、後の問題にかかる計算量を減らします。
具体的には、重さ1の最適解を求め、次に重さ2の最適解を求め、次に…と順番に全探索の容量で計算していきます。
しかし、全探索と異なる点は重さ$$$W_i$$$の最適解を求める際には$$$W_i$$$から品物$$$g_j$$$の重さを引いた$$$W_{i-g_{j}}$$$の最適解$$$V_{i-g_{j}}$$$から$$$V'_{ij}$$$を計算し、この$$$V'_{ij}$$$が最大となる$$$V_i$$$を途中結果として保存（メモ）します。
こうすることにより、全探索のように膨大な計算量になることを回避でき、動的計画法によりナップザック問題の計算量は

$$
O(Wn)
$$

となります。
以下、擬似コードです。

```
Knapsack(W)
    value(0) ← 0
    //容量wを1からWまでループし最適解Vを求めていく
    for i from 1 to W :
        value(i) ← 0
        //品物の数n分ループ
        for j from 1 to n:
            if w[j] ≤ i :
            	//全体の重さがwになったとき、全体の価値が最大になるような品物jを探す
                val ← value(i − w[j] ) + v[j]
                //全体の価値が最大になる場合をvalueにメモする
                if val > value(i):
                    value(i) ← val
    return value(W)
```

```
value(w)
    return max {value(w − w[i]) + v[i]}
```

#####品物が有限の場合 (Knapzack without repetition)
もしn番目の品物を取り、最適解を求めた場合、残りの$$$W-w_n$$$は$$$1, 2, ... , n-1$$$番目の品物を取った最適解である。
もしn番目の品物を取らずに最適解を求めた場合、$$$W$$$は$$$1, 2, ... , n-1$$$番目の品物を取った最適解である。

Wの重さ$$$0 ≤ w ≤ W$$$とn個の品物 $$$0 ≤ i ≤ n$$$を定義し、
$$$value(w , i)$$$を品物1から$$$w$$$を使用し、重さ$$$w$$$を満たす最適解（最大価値）とすると
$$$i$$$番目の品物が使われるか否かにかかわらず、重さ$$$w$$$の最適解は

$$
value(w , i) = max(value(w−w_i , i−1)+v_i, value(w, i−1))
$$

となる。

http://judge.u-aizu.ac.jp/onlinejudge/commentary.jsp?id=DPL_1_B